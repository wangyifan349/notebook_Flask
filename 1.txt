BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer编码器的深度双向预训练语言模型。它的核心理念是：在处理每一个词时，既能利用它左边的上下文信息，也能利用它右边的上下文信息。  

在结构上，BERT仅使用Transformer的编码器部分。每一层编码器由两大模块组成：第一是多头自注意力（multi-head self-attention），它让模型在计算某个词的输出时，可以动态地参考同一句子中所有其他词；第二是前馈神经网络（feed-forward network），负责对注意力输出做非线性变换。每个模块后面都有残差连接（residual connection）和层归一化（layer normalization），以保证梯度稳定和信息流通。BERT-Base包含12层编码器、隐藏维度768、注意力头12个；BERT-Large则有24层、隐藏维度1024、注意力头16个。  

预训练阶段，BERT采用两种自监督学习任务：第一是掩码语言模型（Masked Language Model），在输入中随机选择15%的词，用特殊的[MASK]标记替换80%、保持原词10%、替换为其他随机词10%，模型的目标是根据未被掩码的上下文预测这些被掩码词的原始词汇；第二是下一句预测（Next Sentence Prediction），模型接收两个句子对，一半是真实连贯的上下文句子，一半是随机对接的，任务是判断第二句是否真的是第一句的下一句。掩码语言模型帮助BERT学习双向词级表示，下一句预测则让模型学习句子级别的关系。  

预训练完成后，BERT可以在各种下游任务上进行微调。微调时，通常在BERT顶端添加少量的任务专用层：例如文本分类任务就在[CLS]标记对应的输出向量上接一个全连接层并接Softmax，序列标注任务则在每个位置的输出向量后接分类层。微调阶段会用较低的学习率（通常在2×10⁻⁵到5×10⁻⁵之间）对整个模型进行联合训练，步数一般在几千到几万步之间。  

经过这种“先大规模预训练再少量微调”的策略，BERT在问答系统、情感分析、命名实体识别、自然语言推断、文本摘要等众多自然语言处理任务中都取得了显著的性能提升。它的出现标志着从单向语言模型向双向深度语义表示的关键性进步。
